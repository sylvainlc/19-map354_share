{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan>  Dimension reduction in Python </font>\n",
    "\n",
    "### <font color=darkorange> SVD, Image compression, PCA, Classification, ICA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Singular Value Decomposition applied to image compression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Singular Value Decomposition (SVD) states that for all $\\mathbb{R}^{n \\times d}$ matrix $A$ with rank $r$, there exist $\\sigma_1\\geqslant \\ldots \\geqslant \\sigma_r>0$ such that\n",
    "$$\n",
    "A = \\sum_{k=1}^r \\sigma_k u_k v_k'\\,,\n",
    "$$\n",
    "where $\\{u_1,\\ldots,u_r\\}\\in (\\mathbb{R}^n)^r$ and $\\{v_1,\\ldots,v_r\\}\\in (\\mathbb{R}^d)^r$ are two orthonormal families. The vectors $\\{\\sigma_1,\\ldots,\\sigma_r\\}$ are called singular values of $A$ and $\\{u_1,\\ldots,u_r\\}$ (resp. $\\{v_1,\\ldots,v_r\\}$) are the left-singular (resp. right-singular) vectors of $A$.\n",
    "\n",
    "\n",
    "1. If $U$ denotes the $\\mathbb{R}^{n\\times r}$ matrix with columns given by $\\{u_1,\\ldots,u_r\\}$ and $V$ denotes the $\\mathbb{R}^{p \\times r}$ matrix with columns given by $\\{v_1,\\ldots,v_r\\}$, then the singular value decomposition of $A$ may also be written as\n",
    "$$\n",
    "A = UD_rV'\\,,\n",
    "$$\n",
    "where $D_r = \\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_r)$.\n",
    "\n",
    "\n",
    "2. The singular value decomposition is closely related to the spectral theorem for symmetric semipositive definite matrices. In the framework of this practical session, $A'A$ and $AA'$ are positive semidefinite such that\n",
    "$$\n",
    "A'A = VD_r^2V'\\quad\\mathrm{and}\\quad AA' = UD_r^2U'\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy.linalg.svd function can be used in Python to compute the SVD of a given matrix. The output of this function are:\n",
    "1. $U$ has left singular vectors in the columns ;\n",
    "2. sigma is rank 1 numpy array with singular values ;\n",
    "3. $V$ has right singular vectors in the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Image.open is used to open the input picture (with any .jpg or .png)\n",
    "img = Image.open('./seals.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Image converted into a numpy array or matrix\n",
    "img_mat       = np.array(list(img.getdata(band=0)), float)\n",
    "img_mat.shape = (img.size[1], img.size[0])\n",
    "img_mat       = np.matrix(img_mat)\n",
    "# SVD can then be applied to the matrix img_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function with input the path of an image \"path_image\" and an integer \"k\" \n",
    "# and return in gray scale the reconstructed picture with the first k singular values\n",
    "def svd_decomposition(path_image,k):\n",
    "    img           = Image.open(path_image)\n",
    "    img_mat       = np.array(list(img.getdata(band=0)), float)\n",
    "    img_mat.shape = (img.size[1], img.size[0])\n",
    "    img_mat       = np.matrix(img_mat)\n",
    "    \n",
    "    # Perform Singular Value Decomposition\n",
    "    U, sigma, V = np.linalg.svd(img_mat)\n",
    "    print('Size left singular eigenvectors  ' + str(np.shape(U)))\n",
    "    print('Size right singular eigenvectors  ' + str(np.shape(V)))\n",
    "    print('Size eigenvalues matrix ' + str(np.shape(sigma)))\n",
    "    \n",
    "    # Image reconstruction\n",
    "    reconstimg = np.matrix(U[:, :k]) * np.diag(sigma[:k]) * np.matrix(V[:k, :])\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    plt.plot(sigma[0:12]*100/np.sum(sigma))\n",
    "    plt.title(\"Normalized values of the singular values (in %)\")\n",
    "    fig = plt.figure(2)\n",
    "    plt.title(\"Image reconstruction with %g singular values\"%k)\n",
    "    plt.imshow(reconstimg, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Image reconstruction using interact to analyze the influence of the number of singular values\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "interact(svd_decomposition,path_image='./seals.jpg',k=(1,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Principal Component Analysis </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkred>Application of the SVD to Principal Component Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $(X_i)_{1\\leqslant i\\leqslant n}$ be i.i.d. random variables in $\\mathbb{R}^d$ and consider the matrix $X\\in\\mathbb{R}^{n\\times d}$ such that the $i$-th row of $X$ is the observation $X'_i$. Let $\\Sigma_n$ be the empirical covariance matrix (data are assumed to be centered for simplicity, this can be done manually):\n",
    "$$\n",
    "\\Sigma_n = n^{-1}\\sum_{i=1}^n X_i X'_i\\,.\n",
    "$$\n",
    "Principal Component Analysis  aims at reducing the dimensionality of the observations $(X_i)_{1\\leqslant i \\leqslant n}$ using a \"compression\" matrix $W\\in \\mathbb{R}^{p\\times d}$ with $p\\leqslant d$ so that for each $1\\leqslant i \\leqslant n$, $WX_i$ ia a low dimensional representation of $X_i$. The original observation may then be partially recovered using another matrix $U\\in \\mathbb{R}^{d\\times p}$. Principal Component Analysis computes $U$ and $W$ using the least squares approach:\n",
    "$$\n",
    "(U_{\\star},W_{\\star}) \\in \\hspace{-0.5cm}\\underset{(U,W)\\in \\mathbb{R}^{d\\times p}\\times \\mathbb{R}^{p\\times d}}{\\mathrm{argmin}} \\;\\sum_{i=1}^n\\|X_i - UWX_i\\|^2\\,, \n",
    "$$\n",
    "\n",
    "Let $(U_{\\star},W_{\\star})\\in \\mathbb{R}^{d\\times p}\\times \\mathbb{R}^{p\\times d}$ be a solution to this problem. Then, it can be proved that the columns of $U_{\\star}$ are orthonormal and $W_{\\star} = U_{\\star}'$. Therefore, solving the optimization problem boils down to computing\n",
    "$$\n",
    "U_{\\star} \\in \\hspace{-0.5cm}\\underset{U\\in \\mathbb{R}^{d\\times p}\\,,\\, U'U = I_n}{\\mathrm{argmax}} \\hspace{-.4cm}\\{ \\mathrm{trace}(U'\\Sigma_nU)\\}\\,.\n",
    "$$\n",
    "Let $\\{\\vartheta_1,\\ldots,\\vartheta_d\\}$ be orthonormal eigenvectors associated with the eigenvalues $\\lambda_1\\geqslant \\ldots \\geqslant \\lambda_d$ of $\\Sigma_n$. Then a solution is given by the matrix $U_{\\star}$ with columns $\\{\\vartheta_1,\\ldots,\\vartheta_p\\}$ and $W_{\\star} = U_{\\star}'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkred>Principal Component Analysis as an optimization problem</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any dimension $1\\leqslant p \\leqslant  d$, let $\\mathcal{F}_d^p$ be the set of all vector suspaces of $\\mathbb{R}^d$ with dimension $p$. Principal Component Analysis computes a linear span $V_d$ such as\n",
    "$$\n",
    "V_p \\in \\underset{V\\in \\mathcal{F}_d^p}{\\mathrm{argmin}} \\;\\sum_{i=1}^n\\|X_i - \\pi_V(X_i)\\|^2\\,, \n",
    "$$\n",
    "where $\\pi_V$ is the orthogonal projection onto the linear span $V$. Consequently, $V_1$ is a solution if and only if $v_1$ is solution to:\n",
    "$$\n",
    "v_1 \\in \\underset{v \\in \\mathbb{R}^d\\,;\\, \\|v\\|=1}{\\mathrm{argmax}} \\sum_{i=1}^n   \\langle X_i, v \\rangle^2\\,.\n",
    "$$\n",
    "For all $2\\leqslant p \\leqslant d$, following the same steps, it can be proved that  a solution is given by $V_p = \\mathrm{span}\\{v_1, \\ldots, v_p\\}$ where\n",
    "$$\n",
    "v_1 \\in \\underset{v\\in \\mathbb{R}^d\\,;\\,\\|v\\|=1}{\\mathrm{argmax}} \\sum_{i=1}^n\\langle X_i,v\\rangle^2 \\quad\\mbox{and for all}\\;\\; 2\\leqslant k \\leqslant p\\;,\\;\\; v_k \\in \\underset{\\substack{v\\in \\mathbb{R}^d\\,;\\,\\|v\\|=1\\,;\\\\ v\\perp v_1,\\ldots,v\\perp v_{k-1}}}{\\mathrm{argmax}}\\sum_{i=1}^n\\langle X_i,v\\rangle^2\\,. \n",
    "$$\n",
    "\n",
    "As $V_p = \\mathrm{span}\\{\\vartheta_1, \\ldots, \\vartheta_p\\}$, for all $1\\leqslant i\\leqslant n$,\n",
    "$$\n",
    "\\pi_{V_p}(X_i) = \\sum_{k=1}^p \\langle X_i,\\vartheta_k\\rangle \\vartheta_k  = \\sum_{k=1}^p (X'_i \\vartheta_k)\\vartheta_k = \\sum_{k=1}^p c_k(i)\\vartheta_k\\,,\n",
    "$$\n",
    "where for all $1\\leqslant k \\leqslant p$, the $k$-th principal component is defined as $c_k = X\\vartheta_k$. Therefore the $k$-th principal component is the vector whose components are the coordinates of each $X_i$, $1\\leqslant i\\leqslant n$, relative to the basis $\\{\\vartheta_1, \\ldots, \\vartheta_p\\}$ of $V_p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build a data set with n=100 bi-dimensional i.i.d. data with centered Gaussian distribution\n",
    "X = np.dot(np.random.normal(0,1,[2,2]), np.random.normal(0,1,[2,200])).T\n",
    "plt.scatter(X[:, 0], X[:, 1], s = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform a PCA with one component using PCA(n_components=1) and the function fit.\n",
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "print('The principal component is:')\n",
    "print(pca.components_)\n",
    "\n",
    "print('The explained variance is %g'%pca.explained_variance_)\n",
    "print('The associated singular value is %g'%pca.singular_values_)\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "# X_pca contains the coordinates of each data in the space generated by the principal components\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# in this case pca.components_[k] contains the coordinates of the k-th principal component in\n",
    "# the original space (here the usual Euclidian plane). In a general case pca.components_[k] is a \n",
    "# d-dimensional vector.\n",
    "# X_pca[i] contains the coordinates of the i-th data in the vector space generated by the principal \n",
    "# components.\n",
    "# Therefore, X_pca[i] is a vector with n_components entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a general case, when $\\mathrm{n\\_components} = p$ for all $1\\leqslant i\\leqslant n$ and all $1\\leqslant k \\leqslant p$, the\n",
    "projection of $X_i$ in the space generated by the principal components is:\n",
    "\n",
    "$$\n",
    "\\pi_{V_p}(X_i) = \\sum_{k=1}^{p}X_{\\mathrm{pca}}[i]_k \\times \\mathrm{pca.components\\_}[k]\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform the reduced data set in the original space\n",
    "# X_inverse[i,:] contains the coordinates of the projection of Xi in the original space\n",
    "X_inverse = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2, s=10)\n",
    "plt.scatter(X_inverse[:, 0], X_inverse[:, 1], alpha=0.8, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data must be centered and scaled when the variables have different units, such as height (cm) and weight (kg). \n",
    "# This prevents the main directions from being governed by one or more variables with a higher variance than the \n",
    "# other variables. See the illustration below.\n",
    "# When the variables have the same unit, the reduction should be considered on a case-by-case basis. \n",
    "# Sometimes, it is not advised to force variances to be equal (for example, with respect to grades, we may want to discriminate \n",
    "# more against students in relation to a subject whose grades vary more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = np.dot(np.random.normal(0,1,[3,3]), np.random.normal(0,1,[3,200])).T\n",
    "X[:,0] = 10*X[:,0] + 15 \n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "principalDf = pd.DataFrame(data = X_pca, columns = ['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('PCA with two principal components', fontsize = 20)\n",
    "ax.scatter(principalDf['Principal Component 1'], principalDf['Principal Component 2'], s = 10)\n",
    "ax.grid()\n",
    "\n",
    "print('The first principal component explains %f of the variance'%pca.explained_variance_ratio_[0])  \n",
    "print('The second principal component explains %f of the variance'%pca.explained_variance_ratio_[1]) \n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "principalComponents = pca.fit_transform(X_scaled) \n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('PCA with two principal components', fontsize = 20)\n",
    "ax.scatter(principalDf['Principal Component 1'], principalDf['Principal Component 2'], s = 10)\n",
    "ax.grid()\n",
    "plt.axis('equal');\n",
    "\n",
    "print('The first principal component explains %f of the variance'%pca.explained_variance_ratio_[0])  \n",
    "print('The second principal component explains %f of the variance'%pca.explained_variance_ratio_[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you perform a standardized PCA on a huge number of variables: the percentage of variability of the two first dimensions\n",
    "# is not necessarily small. In the case of independent Gaussian variables, the percentage of variability decreases with the \n",
    "# dimension. If the data set is made by a copy of the same variable in each column, the percentage variability of the two first\n",
    "# dimensions will then be 100% regardless of the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a scaled PCA has been performed with 4 data sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./PCA_cor.png\" width=\"500\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible to link these correlation matrices with the PCA results.\n",
    "# The value of the first eigenvalue approximately gives the number of correlated \n",
    "# variables explained by the first dimension.\n",
    "\n",
    "# The matrix A seems to be a block diagonal matrix with blocks of size $3x3$ and $2x2$. \n",
    "# The block of size $2x2$ is close to the matrix with only ones (which is of rank 1) thus \n",
    "# one eigenvalue must be close to zero. This corresponds to PCA 4.\n",
    "\n",
    "# The matrix B is has all its entries near one, thus it is close to a matrix of\n",
    "# rank one. If this matrix was of rank one, the variance could be explained with\n",
    "# the first component only. In that case, the inertia associated with the first\n",
    "# component would be $5$ and $0$ for the other components. This corresponds to PCA 1.\n",
    "\n",
    "# The matrix D is close to the identity. The PCA applied to the identity would\n",
    "# give the same inertia to all components (inertia close to 1). This corresponds\n",
    "# to PCA 2.\n",
    "\n",
    "# The matrix C is similar to B but the correlation are weaker. Thus the first\n",
    "# component has a high inertia. This corresponds to PCA 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the graph below, it is possible to give roughly the percentage of variability explained by the first dimension\n",
    "# and by the first plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./AnaDo_ACP_exo_Graphe_var2.jpg\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of variables aligned with the first two dimensions should be divided by the total number of variables. \n",
    "# The first dimension explains 44% of the variability and the first plane 77%. The percentage of explained variance is equal to the sum of square norms of\n",
    "# projection of variables along the first axis. Variables V5, V6, V9 are orthogonal to the first axis and variables V7 and V8 are not well represented by\n",
    "# the first plan (and thus are orthogonal to this plan and in particular to the first axis). Roughly, the norm of V1, V2, V3 and V4 is one which leads to a\n",
    "# percentage of variance for the first axis equal to $4/9$. Similarly, for the first plan, we have a percentage of variance \n",
    "# equal to $7/9$ since the norm of $V5$ and $V9$ is close to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> PCA on a real data set </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section focuses on a data set where each observation is a global confort given by an employee \n",
    "# in a large building (related to temperature, noise, CO2 level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data about confort at workplace \"data_confort.csv\"\n",
    "df = pd.read_csv('data_confort.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data rescaling using min_max_scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "array          = df.values \n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_scaled       = min_max_scaler.fit_transform(array)\n",
    "X              = (pd.DataFrame(x_scaled, columns = df.columns))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each category is defines as follows if Class_i = 1 it means that the confort is affected either by \n",
    "# CO2 level, humidity, temperature or noise level at the time of the data collection.\n",
    "# if all Class_i = 0, this means that the confort is good enough (category 0).\n",
    "categories = []\n",
    "for i in df.iloc[:,-4:].values:\n",
    "    if np.array_equal(i, [0,0,0,0]):\n",
    "        categories.append(0) \n",
    "    \n",
    "    elif np.array_equal(i, [0,0,0,1]):\n",
    "        categories.append(1) \n",
    "    \n",
    "    elif np.array_equal(i, [0,0,1,0]):\n",
    "        categories.append(2) \n",
    "    \n",
    "    elif np.array_equal(i, [0,1,0,0]):\n",
    "        categories.append(3) \n",
    "    \n",
    "    else :\n",
    "        categories.append(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories Class_1, Class_2, etc. are replaced by a single category in {0,1,2,3,4} \n",
    "X['categories'] = np.array(categories)\n",
    "X = X.drop([\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means of the features grouped by categories using the function groupby\n",
    "X.groupby([\"categories\"]).mean().plot(kind='bar',figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the correlations of the features and the categories using heatmap\n",
    "import seaborn as sns\n",
    "corr = X.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean table by category. \n",
    "# the function apply is used to compute the mean value of each feature in each category with respect \n",
    "# to its mean value in the best confort category.\n",
    "mean_table = X.groupby(['categories']).mean().T\n",
    "index_list = mean_table.index.values\n",
    "mean_table = pd.DataFrame(mean_table.values, index=index_list,columns=['5','4','3','2','1'])\n",
    "mean_table[\"4 % 5\"] = mean_table.apply(lambda x : (x.iloc[1] - x.iloc[0]) / x.iloc[0]*100,axis=1 )\n",
    "mean_table[\"3 % 5\"] = mean_table.apply(lambda x : (x.iloc[2] - x.iloc[0]) / x.iloc[0]*100,axis=1 )\n",
    "mean_table[\"2 % 5\"] = mean_table.apply(lambda x : (x.iloc[3] - x.iloc[0]) / x.iloc[0]*100,axis=1 )\n",
    "mean_table[\"1 % 5\"] = mean_table.apply(lambda x : (x.iloc[4] - x.iloc[0]) / x.iloc[0]*100,axis=1 )\n",
    "mean_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the associated values\n",
    "mean_table.iloc[:,-4:].groupby(index_list).mean().plot(kind='bar',figsize=(15,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Provide boxplots of some features among categories</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some boxplots displaying the distribution of a feature within each category.\n",
    "# This may help to detect useless features for classification... or a feature highly impacted by the category.\n",
    "X.boxplot('Feat_11', by=['categories'],figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.boxplot('Feat_3', by=['categories'],figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a PCA with two components (after data scaling)\n",
    "from sklearn import preprocessing \n",
    "norm_pca = preprocessing.scale(X)\n",
    "n_comp = 6\n",
    "pca = PCA(n_components = n_comp)\n",
    "\n",
    "x = norm_pca[:,:-1]\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf         = pd.DataFrame(data = principalComponents, columns = ['Princ. Comp. ' + str(i) for i in range(n_comp)])\n",
    "\n",
    "finalDf = pd.concat([principalDf, X.categories], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> Use of PCA before Machine learning applications (classification here)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# provide a clustering of the in the plane given by the first two principal components (for illustration purposes)\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "targets_names = ['5','4','3','2','1']\n",
    "targets = [0,1,2,3,4]\n",
    "\n",
    "for target in targets:\n",
    "    indicesToKeep = finalDf['categories'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'Princ. Comp. 0'], finalDf.loc[indicesToKeep, 'Princ. Comp. 1'],s=10)\n",
    "\n",
    "ax.legend(targets_names)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Split the dataset using 70% of the data to learn a classifier and 30% to test the classifier performance.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, Y_train , Y_test = train_test_split(X.iloc[:,:-1] , X.iloc[:,-1] , test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Use a PCA with 2 components before learning a Support Vector Machine classifier (with SVC) on the training set. Compute the classification score.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_transform = pca.transform(X_train)\n",
    "X_test_transform  = pca.transform(X_test)\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train_transform, Y_train)\n",
    "end_t = time.time()\n",
    "print('Classification score %g'%clf.score(X_test_transform, Y_test))\n",
    "print('Computational time %g sec.'%(end_t-start_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Compare the results for several number of components using cross validation.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = np.zeros(10)\n",
    "comp_time  = np.zeros(10)\n",
    "for n_comp in np.arange(2,12):\n",
    "    start_t = time.time()\n",
    "    pca     = PCA(n_components=n_comp)\n",
    "\n",
    "    pca.fit(X.iloc[:,:-1])\n",
    "\n",
    "    X_transform          = pca.transform(X.iloc[:,:-1])\n",
    "    kfold                = StratifiedKFold(n_splits=5, shuffle=True) \n",
    "    cv_results           = cross_val_score(clf, X_transform, X[\"categories\"], cv=kfold)\n",
    "    comp_time[n_comp-2]  = time.time()-start_t\n",
    "    test_score[n_comp-2] = cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (10, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(2,12),test_score, '-')\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Cross validation score')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(2,12),comp_time, '-')\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Computational time (s)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> Independent component analysis (ICA)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "time_steps = np.linspace(0,150,300)\n",
    "\n",
    "# Sources\n",
    "S = np.array([signal.sawtooth(time_steps),\n",
    "              np.sin(3*time_steps)]).T\n",
    "\n",
    "A = np.array([[0.5, 3.0],\n",
    "              [1.2, 1.6]])\n",
    "\n",
    "# Observed signal\n",
    "X = S.dot(A).T\n",
    "\n",
    "plt.plot(time_steps, X[0], lw = 1)\n",
    "plt.plot(time_steps, X[1], lw = 1)\n",
    "plt.title('observed signals', fontsize = 15)\n",
    " \n",
    "fig = plt.figure()\n",
    "plt.plot(time_steps, S, lw = 1)\n",
    "plt.title('Unknown sources', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing proceeds in two steps. The first step amounts to `centering` the data. For all $1\\leqslant i \\leqslant d$,\n",
    "\n",
    "$$\\tilde x_{ij} = x_{ij}-{\\frac {1}{n}}\\sum _{\\ell = 1}^nx_{i\\ell}\\,.$$\n",
    "\n",
    "Then, a `linear transformation is used to transform data into a new dataset with unit covariance matrix`.  A standard approach to do so is to perform an eigenvalue decomposition of the empirical covariance matrix $\\Sigma$ of the centered data $\\tilde X = (\\tilde x_{ij})_{1\\leqslant i\\leqslant d, 1\\leqslant j\\leqslant n}$:  \n",
    "$$\\Sigma = P \\Delta P^{-1}\\,,$$\n",
    "where $\\Delta$ is the diagonal matrix containing all the eigenvalues of $\\Sigma$. The unit covariance covariance data is the given by:  \n",
    "$$\n",
    "\\widehat X= \\Delta^{-1/2}P^{T}\\tilde X\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "\n",
    "# Center data\n",
    "Xmean     = np.mean(X, axis=1, keepdims=True)\n",
    "Xcentered =  X - Xmean\n",
    "\n",
    "# Transform data into a new dataset with unit covariance matrix\n",
    "# In the case of a square matrix, an eigenvalue decomposition is equivalent to a SVD.\n",
    "Sigma         = np.cov(Xcentered)\n",
    "U, S, V       = np.linalg.svd(Sigma)\n",
    "deltainv      = np.diag(1.0 / np.sqrt(S))\n",
    "weight_matrix = np.dot(U, np.dot(deltainv, U.T))\n",
    "Xrescaled     = np.dot(weight_matrix, Xcentered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_ICA(signals, W, m, nb_it):\n",
    "    W = W_init.copy()\n",
    "    for c in range(m):        \n",
    "        wc = W[c, :].copy()\n",
    "        wc = wc.reshape(m, 1)\n",
    "        wc = wc / np.sqrt(np.sum(wc ** 2))\n",
    "        for i in range(nb_it):\n",
    "            # Compute intermediate weights to \"maximize non-Gaussianity\"\n",
    "            wX = np.dot(wc.T, signals)\n",
    "            \n",
    "            w1 = np.tanh(wX).T\n",
    "            w2 = (1 - np.square(np.tanh(wX)))\n",
    "\n",
    "            # Weight update\n",
    "            w_new = (signals * w1.T).mean(axis = 1) - np.mean(w2) * wc.squeeze()\n",
    "            w_new = w_new - np.dot(np.dot(w_new, W[:c].T), W[:c])\n",
    "            w_new = w_new / np.sqrt(np.sum(w_new ** 2))\n",
    "            \n",
    "            wc = w_new\n",
    "\n",
    "        W[c, :] = wc.T\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "m      = 2\n",
    "W_init = np.random.rand(m, m)\n",
    "nb_it  = 100\n",
    "\n",
    "W = fast_ICA(Xrescaled, W_init, m, nb_it)\n",
    "\n",
    "recoveredX = Xrescaled.T.dot(W.T)\n",
    "recoveredX = (recoveredX.T - Xmean).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(time_steps, S[:,0], label='First source', lw = 1)\n",
    "plt.plot(time_steps, S[:,1], label='Second source', lw = 1)\n",
    "plt.plot(time_steps, recoveredX[:,0], '--', label='First recovered signal', lw = 1)\n",
    "plt.plot(time_steps, recoveredX[:,1], '--', label='Second recovered signal', lw = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
