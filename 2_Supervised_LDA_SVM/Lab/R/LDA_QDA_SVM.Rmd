---
title: "Linear Discriminant Analysis"
date: 
output:
  pdf_document: default
  html_document: default
---

```{r global_Options, echo=TRUE, include = FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, include = TRUE, eval=FALSE)
```


```{r, include=FALSE}
#Pour convertir le Rmarkdown en un fichier R, on peut utiliser la commande suivante dans la console
#knitr::purl("01_Logistic.Rmd")
```



```{r, echo=TRUE, include=TRUE, eval = TRUE}
library(MASS)
library(matlib) 
library(ggplot2)
library(gridExtra)
library(grid)
library(klaR)
```

This lab sets the focus on Linear Discriminant Analysis (LDA), a two-class classification algorithm using the library $\texttt{MASS}$. 

Let $(X,Y)$ be a couple of random variables with values in $\mathbb{R}^2 \times \{0,1\}$ and satisfying, for all $k\in\{0,1\}$ and all $A\in \mathcal{B}(\mathbb{R}^2)$,
\begin{equation}
\label{eq:lda:mixture}
\mathbb{P}(Y=k)=\pi_{k}>0 \quad\textrm{and}\quad \mathbb{P}(X\in A|Y=k)=
\int_A g_{k}(x)\,\mathrm{d}x,
\end{equation}
where $\pi_{0}+\pi_{1}=1$ and $g_0, g_1$ are two probability densities in
$\mathbb{R}^2$. In this lab, it is assumed that the conditional distribution of $X$ given $Y$ is Gaussian, i.e., for $k\in\{0,1\}$,
$$
g_{k}: x \mapsto \det(2\pi\Sigma_k)^{-1/2}\exp\left(-{1\over 2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right)\,,
$$
with $\Sigma_0$, $\Sigma_1$ symmetric definite positive matrices and $\mu_0,\mu_1\in \mathbb{R}^2$,  $\mu_0\ne\mu_1$.

**1. Assume that $\pi_0 = \pi_1 = 0.5$, $\Sigma_0 = \Sigma_1 = I_2$, $\mu_1 = (0,1)$ and $\mu_2 = (5,10)$. Simulate $n=1000$ independent couples $(X_i, Y_i)$ distributed as $(X,Y)$. Plot these data, by assigning different colors to data labelled as $0$ or $1$.** 

```{r, echo =TRUE, include=TRUE, eval=TRUE}
# Data generation 
n       <- 1000
mu_0    <- c(0,1)
mu_1    <- c(5,10)
pi_0    <- 0.5
Sigma_0 <- diag(2)
Sigma_1 <- diag(2)

draw_rv = function(x, pi_0) {
  y = rbinom(1, 1, prob = 1 - pi_0)
  if (y == 0) {
    Z <- mvrnorm(1, mu_0, Sigma_0)
    return(c(Z, 0))
  }
  else {
    Z <- mvrnorm(1, mu_1, Sigma_1)
    return(c(Z, 1))
  }
}
dataset           = data.frame(t(sapply(c(1:n), FUN = draw_rv, pi_0)))
colnames(dataset) = c("X1", "X2", "Class")
dataset$Class     <- as.factor(dataset$Class)
dataset           = dataset[sample(1:n),]
pal               <-c("indianred","steelblue4")
p1<-ggplot(dataset,aes(x = X1,y = X2, color = Class)) + geom_point() + theme_minimal() + scale_color_manual(values=pal)
p1
```

**2. What is the distribution of $X$? **

For all $A\in \mathcal{B}(\mathbb{R}^2)$,
\begin{align*}
\mathbb{P}(X \in A) & = \mathbb{P}(Y = 0)\mathbb{P}(X \in A | Y=0) + \mathbb{P} (Y = 1) \mathbb{P}(X \in A | Y=1)\,,\\
& = \int_A(\pi_0 g_0 (x) + \pi_1 g_1(x)) \mathrm{d}x.
\end{align*}

**3. Define the classifier $h^{\star}:\mathbb{R}^2\to\{0,1\}$ as follows. For all  $x\in\mathbb{R}^2$, **
$$h^{\star}(x)={\bf 1}_{\{\pi_{1}g_{1}(x)>\pi_{0}g_{0}(x)\}} \, .$$
**Prove that the classifier $h^{\star}$ fulfills**
$$\mathbb{P}(h^{\star}(X)\neq Y)= \underset{h: \mathbb{R}^2\to\{0,1\}}{\min}\,\left\{\mathbb{P}(h(X)\neq Y)\right\}\,.$$
The Bayes classifier $h^{\star}$ minimizes the risk $\mathbb{P} (h(X) \neq Y)$ among all classifiers and is given by
\begin{align*}
h^{\star}(x) = \left\lbrace
\begin{array}{ll}
0 & \textrm{if}~\eta(x) \leqslant 1/2\\
1 & \textrm{if}~\eta(x) > 1/2\,,
\end{array}
\right.
\end{align*}
where $\eta: x\to  \mathbb{P}(Y=1|X)_{X=x}$. In the case of LDA, 
$$
\mathbb{P}(Y=1|X)_{|X=x} =  \frac{\pi_1 g_1(x)}{\pi_0 g_0(x) + \pi_1 g_1(x)}\,,
$$
and the condition $\eta(x) \leqslant 1/2$ can be rewritten as
\begin{align*}
& \frac{\pi_1 g_1(x)}{\pi_0 g_0(x) + \pi_1 g_1(x)} \leqslant 1/2\,,
\end{align*}
that is $\pi_1 g_1(x) \leqslant \pi_0 g_0(x)$. Consequently, $h^{\star}(x) = \mathbb{1}_{\pi_1 g_1(x) > \pi_0 g_0(x)} = h_*(x)$, which proves that $h_*$ is the Bayes classifier. 

**4. General case. Assume that $\Sigma_0 = \Sigma_1 = \Sigma$ is nonsingular, and that $\mu_0 \neq \mu_1$. Prove that for all $x\in\mathbb{R}^2$, $\pi_{1}g_{1}(x)>\pi_{0}g_{0}(x)$ is equivalent to**
$$(\mu_{1}-\mu_{0})^T\Sigma^{-1}\left(x-{\mu_{1}+\mu_{0}\over
2}\right)>\log(\pi_{0}/\pi_{1}).$$ **Interpret geometrically this result.**
By definition of the Bayes classifier:
\begin{align*}
\pi_1 g_1(x) > \pi_0 g_0(x)& \\
\Leftrightarrow & \log(\pi_1 g_1(x)) > \log(\pi_0 g_0(x))\\
\Leftrightarrow & -{1\over 2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) + {1\over 2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0) > \log(\pi_0/ \pi_1)\\
\Leftrightarrow		& -{1\over 2}\Big(  -\mu_1^T\Sigma^{-1}x + \mu_1^T\Sigma^{-1}\mu_1  - x^T\Sigma^{-1}\mu_1  +\mu_0^T\Sigma^{-1}x - \mu_0^T\Sigma^{-1}\mu_0 + x^T\Sigma^{-1}\mu_0             \Big)  > \log(\pi_0/ \pi_1)\\
\Leftrightarrow		&\, x^T\Sigma^{-1}\mu_1 - x^T\Sigma^{-1}\mu_0 - \frac{1}{2} \mu_1^T\Sigma^{-1}\mu_1 + \frac{1}{2} \mu_0^T\Sigma^{-1}\mu_0  > \log(\pi_0/ \pi_1)\\
\Leftrightarrow		& (\mu_1 - \mu_0)^T \Sigma^{-1} \Big(x - \frac{\mu_1 + \mu_0}{2}\Big)  > \log(\pi_0/ \pi_1).
\end{align*}


5. Apply this result to the data simulated in question 1 and plot the boundary decision of classifier $h^{\star}$, i.e. the line which separates $\{x \in \mathbb{R}^2, h^{\star}(x) = 0 \}$ from $\{x \in \mathbb{R}^2, h^{\star}(x) = 1 \}$.


```{r, echo = TRUE, eval = TRUE, include = TRUE}
boundary_true_parameters = function(x, mu0, mu1, Sigma, pi0){
  u = t(as.matrix(mu1-mu0)) %*% inv(Sigma)
  v = (u %*% (matrix(x - ((mu1+mu0)/2)) )) - log(pi0/(1-pi0))
  return(as.numeric(v)) 
}

list_x1 <- seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2 <- seq(min(dataset$X2), max(dataset$X2),length=100)
u       <- expand.grid(list_x1, list_x2)

dataforcontour  <- data.frame(list_x1=u$Var1,list_x2=u$Var2)
boundary_values <- apply(dataforcontour, MARGIN = 1, FUN = boundary_true_parameters, mu0=mu_0, mu1=mu_1, Sigma= Sigma_1, pi0 = pi_0)
dataforcontour$boundary_values<-boundary_values

pal <-c("indianred","steelblue4")
p2<-ggplot(dataset,aes(x = X1,y = X2, color = Class)) +
  geom_point() +
geom_contour(data=dataforcontour,aes(x = list_x1,y = list_x2, z = boundary_values), breaks=0, inherit.aes = FALSE) + 
  theme_minimal() +
  scale_color_manual(values=pal)
p2
```

6. In real learning applications, the parameters ($\Sigma_0, \Sigma_1, \mu_0, \mu_1, \pi_0, \pi_1$) are unknwon. Propose a way to use an indepent sample $\{(X_i,Y_i)\}_{1\leqslant i\leqslant n}$, where for all $1\leqslant i\leqslant n$, $(X_i,Y_i)$ has the same distribution as $(X,Y)$, to estimate these parameters.  Implement question 5 in the case of unknown parameters. Compare the classifiers of question 5 and 6. Comment.

```{r, echo = TRUE, eval = TRUE, include = TRUE}
est_pi_0 = length(which(dataset$Class==0))/dim(dataset)[1]
est_pi_1 = 1 - est_pi_0

est_mu_0 = colMeans(dataset[which(dataset$Class==0),c(1,2)])
est_mu_1 = colMeans(dataset[which(dataset$Class==1),c(1,2)])

est_Sigma_0 = cov(dataset[which(dataset$Class==0),c(1,2)], dataset[which(dataset$Class==0),c(1,2)])

est_Sigma_1 = cov(dataset[which(dataset$Class==1),c(1,2)], dataset[which(dataset$Class==1),c(1,2)])


list_x1 <- seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2 <- seq(min(dataset$X2), max(dataset$X2),length=100)
u       <- expand.grid(list_x1, list_x2)

dataforcontour  <- data.frame(list_x1=u$Var1,list_x2=u$Var2)
boundary_values <- apply(dataforcontour, MARGIN = 1, FUN = boundary_true_parameters, mu0=est_mu_0, mu1=est_mu_1, Sigma= est_Sigma_0, pi0 = est_pi_0)
dataforcontour$boundary_values<-boundary_values

pal <-c("indianred","steelblue4")
p3<-ggplot(dataset,aes(x = X1,y = X2, color = Class)) +
  geom_point() +
geom_contour(data=dataforcontour,aes(x = list_x1,y = list_x2, z = boundary_values), breaks=0, inherit.aes = FALSE) + 
  theme_minimal() +
  scale_color_manual(values=pal)
p3
```

7. Sample a new dataset as in question 1 but with $\Sigma_0 \neq I_2$ (a covariance matrix must be symmetric positive-definite). 

```{r, echo = TRUE, eval = TRUE, include = TRUE, cache = TRUE}
#Data generation 
n = 1000
mu_0 = c(0,1)
mu_1 = c(5,10)
pi_0 = 0.5
Sigma_0 = matrix(c(1,0,0,1), nrow = 2, ncol = 2)

A <- matrix(runif(4), ncol=2) 
Sigma_1 <- t(A) %*% A
det(Sigma_1)

draw_rv = function(j, pi_0){
  y = rbinom(1,1,prob=1-pi_0)
  if (y==0) {
    return(c(mvrnorm(1,mu_0, Sigma_0),0))
  }
  else {return(c(mvrnorm(1,mu_1, Sigma_1),1))}
}

dataset = as.data.frame(t(sapply(c(1:n), FUN = draw_rv, pi_0 = 0.5)))
colnames(dataset) = c("X1", "X2", "Class")
dataset$Class <- as.factor(dataset$Class)
dataset = dataset[sample(1:n),]
p4<-ggplot(dataset,aes(x = X1,y = X2, color = Class)) + geom_point() + theme_minimal() + scale_color_manual(values=pal)
p4
```


8. Since the covariance matrices are distinct, the boundary found in question 4 is not valid anymore. Prove that for all $x\in\mathbb{R}^2$, $\pi_{1}g_{1}(x)>\pi_{0}g_{0}(x)$ is equivalent to

\begin{align*}
x^T\tilde{\Sigma} x + u^Tx + c >0,
\end{align*}
where
\begin{align*}
\tilde{\Sigma} & = \frac{\Sigma_0^{-1} - \Sigma_1^{-1}}{2}, \\
u & = \Sigma_1^{-1} \mu_1 - \Sigma_0^{-1} \mu_0\\
c & = \frac{1}{2}\big( \mu_0^T\Sigma_0^{-1} \mu_0 - \mu_1^T\Sigma_1^{-1} \mu_1) - \log \Big(\frac{\pi_0}{1 - \pi_0}\Big) - 0.5\log(| \det(\Sigma_1) | ) + 0.5\log(| \det(\Sigma_2) | ).
\end{align*}

If we do not assume anymore that $\Sigma_0 = \Sigma_1$, then the decision boundary is given by a quadric: the term $x^T\Sigma_1x - x^T\Sigma_0x$ in question $3$ does not cancel anymore. More precisely, \begin{align*}
& \pi_1 g_1(x) > \pi_0 g_0(x) \\
\Leftrightarrow & \log(\pi_1 g_1(x)) > \log(\pi_0 g_0(x))\\
\Leftrightarrow & -{1\over 2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + {1\over 2}(x-\mu_0)^T\Sigma_0^{-1}(x-\mu_0) > \log(\pi_0/ \pi_1) + 0.5\log(| \det(\Sigma_1) | ) - 0.5\log(| \det(\Sigma_2) | )\\
\Leftrightarrow		& -{1\over 2}\Big(x^T \Sigma_1^{-1} x - x^T \Sigma_0^{-1} x   -\mu_1^T\Sigma^{-1}x + \mu_1^T\Sigma^{-1}\mu_1  - x^T\Sigma^{-1}\mu_1  +\mu_0^T\Sigma^{-1}x  - \mu_0^T\Sigma^{-1}\mu_0 + x^T\Sigma^{-1}\mu_0             \Big)
- \log(\pi_0/ \pi_1) \\
& \qquad - 0.5\log(| \det(\Sigma_1) | ) + 0.5\log(| \det(\Sigma_2) | ) >0\\
\Leftrightarrow		& \frac{1}{2} x^T \Big( \Sigma_0^{-1} - \Sigma_1^{-1} \Big) x + \Big( \mu_1^T \Sigma_1^{-1} - \mu_0^T \Sigma_0^{-1} \Big) x  + \frac{1}{2} \Big( \mu_0^T \Sigma_0^{-1} \mu_0 - \frac{1}{2} \mu_1^T \Sigma_1^{-1} \mu_1\Big) - \log\Big(\frac{\pi_0}{1 - \pi_0}\Big)  \\
& \qquad - 0.5\log(| \det(\Sigma_1) | ) + 0.5\log(| \det(\Sigma_2) | ) >0,
\end{align*} 
which gives the desired result. 



9. As in question 5, plot the previous boundary decision with the true parameter values. 

```{r, echo = TRUE, include= TRUE, eval = TRUE}
boundary_true_parameters_quadratic = function(x, mu0, mu1, Sigma0, Sigma1, pi0){
  
  u1 =  -0.5*(t(as.matrix(x-mu1))) %*% inv(Sigma1) %*% as.matrix((x - mu1))
  u0 =  0.5*(t(as.matrix(x-mu0))) %*% inv(Sigma0) %*% as.matrix((x - mu0))
  cste = - log(pi0/(1-pi0))
  bonus = -0.5*log(abs(det(Sigma1))) + 0.5*log(abs(det(Sigma0)))
  
  return(as.numeric(u1+u0+cste+bonus)) 
}

list_x1 <- seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2 <- seq(min(dataset$X2), max(dataset$X2),length=100)
u       <- expand.grid(list_x1, list_x2)

dataforcontour  <- data.frame(list_x1=u$Var1,list_x2=u$Var2)
boundary_values <- apply(dataforcontour, MARGIN = 1, FUN = boundary_true_parameters_quadratic, mu0=mu_0, mu1=mu_1, Sigma0 = Sigma_0, Sigma1 = Sigma_1, pi0 = pi_0)
dataforcontour$boundary_values<-boundary_values

pal <-c("indianred","steelblue4")
p5<-ggplot(dataset,aes(x = X1,y = X2, color = Class)) +
  geom_point() +
geom_contour(data=dataforcontour,aes(x = list_x1,y = list_x2, z = boundary_values), breaks=0, inherit.aes = FALSE) + 
  theme_minimal() +
  scale_color_manual(values=pal)
p5
```


10. Apply the previous question when all parameters are replaced by their maximum likelihood estimates. 

```{r, echo = TRUE, eval = TRUE, include = TRUE}
est_pi_0 = length(which(dataset$Class==0))/dim(dataset)[1]
est_pi_1 = 1 - est_pi_0

est_mu_0 = colMeans(dataset[which(dataset$Class==0),c(1,2)])
est_mu_1 = colMeans(dataset[which(dataset$Class==1),c(1,2)])

est_Sigma_0 = cov(dataset[which(dataset$Class==0),c(1,2)], dataset[which(dataset$Class==0),c(1,2)])

est_Sigma_1 = cov(dataset[which(dataset$Class==1),c(1,2)], dataset[which(dataset$Class==1),c(1,2)])

list_x1 <- seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2 <- seq(min(dataset$X2), max(dataset$X2),length=100)
u       <- expand.grid(list_x1, list_x2)

dataforcontour  <- data.frame(list_x1=u$Var1,list_x2=u$Var2)
boundary_values <- apply(dataforcontour, MARGIN = 1, FUN = boundary_true_parameters_quadratic,mu0=est_mu_0, mu1=est_mu_1, Sigma0 = est_Sigma_0, Sigma1 = est_Sigma_1, pi0 = est_pi_0)
dataforcontour$boundary_values<-boundary_values

pal <-c("indianred","steelblue4")
p6<-ggplot(dataset,aes(x = X1,y = X2, color = Class)) +
  geom_point() +
geom_contour(data=dataforcontour,aes(x = list_x1,y = list_x2, z = boundary_values), breaks=0, inherit.aes = FALSE) + 
  theme_minimal() +
  scale_color_manual(values=pal)
p6
```
```{r}
g<-arrangeGrob(p2,p3,ncol=2,nrow=1)
ggsave(file="lda_plot.png", g)
g<-arrangeGrob(p5,p6,ncol=2,nrow=1)
ggsave(file="qda_plot.png", g)

```

11. Apply a quadratic discriminant analysis to the 65% of the data contained in the iris dataset with the function qda from the MASS package. Provide the estimated means and prior probabilities for each class.
```{r, echo=TRUE, include=TRUE, eval = TRUE}
data_iris   <- sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.65,0.35))
train_data  <- iris[data_iris, ]
test_data   <- iris[!data_iris, ]
head(train_data)

qda.iris <- qda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, train_data)
qda.iris 
```
12.  Compute the contingency table associated with the predition results on the test dataset. The function partimat may also be used to display a multiple figure array which shows the classification of observations for every combination of two variables
```{r, echo=TRUE, include=TRUE, eval = TRUE}
#partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train_data, method="qda")
qda.test <- predict(qda.iris,test_data)
test_data$qda <- qda.test$class
table(test_data$qda,test_data$Species)
```

13. Load the package ISLR to perform a SVM analysis to the Khan training data. Use the SVM function with a kernel function. Give the contingency table for the training dataset and for the test data. Show that the results depend on the kernel function used to learn the classifier.
```{r, echo=TRUE, include=TRUE, eval = TRUE}
library(ISLR)
library(e1071)
khan_frame <- data.frame(x=Khan$xtrain, y=as.factor (Khan$ytrain ))
khan_svm   <- svm(y~., data = khan_frame, kernel = "radial")

summary(khan_svm)

# Contingency table for the training dataset
table(khan_svm$fitted , khan_frame$y)

data_test = data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
# Predict labels with the output of the SVM 
pred_test = predict (khan_svm  , newdata =data_test)
# Contingency table for the test data
table(pred_test, data_test$y)
```
14. Compare the performance of kernel SVM and LDA on the postal codes dataset: USPS358.

