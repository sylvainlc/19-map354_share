{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan>  Expectation Maximization for latent data models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where we are interested in estimating unknown parameters $\\theta\\in\\mathbb{R}^m$ characterizing a model with missing data, the Expectation Maximization (EM) algorithm (Dempster et al. 1977) can be used when the joint distribution of the missing data $X$ and the observed data $Y$ is explicit. For all $\\theta\\in\\mathbb{R}^m$, let $p_{\\theta}$ be the probability density function of $(X,Y)$ when the model is parameterized by $\\theta$ with respect to a given reference measure $\\mu$. The EM algorithm aims at computing iteratively an approximation of the maximum likelihood estimator which maximizes the observed data loglikelihood:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta;Y) = \\log p_{\\theta}(Y) =\\log \\int p_{\\theta}(x,Y)\\mu(\\mathrm{d}x)\\,.\n",
    "$$\n",
    "\n",
    "As this quantity cannot be computed explicitly in general cases, the EM algorithm finds the maximum likelihood estimator by iteratively maximizing the expected complete data loglikelihood.\n",
    " \n",
    "Start with an inital value $\\theta^{(0)}$ and let $\\theta^{(t)}$ be the estimate at the $t$-th iteration for $t\\geqslant 0$, then the next iteration of EM is decomposed into two steps.\n",
    "\n",
    "1. E-step. Compute the expectation of the complete data loglikelihood, with respect to the conditional distribution of the missing data given the observed data parameterized by $\\theta^{(t)}$:\n",
    "\n",
    "$$\n",
    "Q(\\theta,\\theta^{(t)}) =\\mathbb{E}_{\\theta^{(t)}}\\left[\\log p_{\\theta}(X,Y)|Y \\right]\\,.\n",
    "$$\n",
    "\n",
    "2. M step. Determine $\\theta^{(t+1)}$ by maximizing the function Q:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)}\\in \\mbox{argmax}_\\theta Q(\\theta,\\theta^{(t)})\\,.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Prove the following crucial property motivates the EM algorithm.  For all $\\theta,\\theta^{(t)}$,\n",
    "    \n",
    "$$\n",
    "\\ell(Y;\\theta) - \\ell(Y;\\theta^{(t)}) \\geqslant Q(\\theta,\\theta^{(t)})-Q(\\theta^{(t)},\\theta^{(t)})\\,.\n",
    "$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be proved by noting that\n",
    "\n",
    "$$\n",
    "\\ell(Y;\\theta) = \\log \\left(\\frac{p_{\\theta}(X,Y)}{p_{\\theta}(X|Y)}\\right)\\,.\n",
    "$$\n",
    "\n",
    "Considering the conditional expectation of both terms given $Y$ when the parameter value is $\\theta^{(t)}$ yields\n",
    "\n",
    "$$\n",
    "\\ell(Y;\\theta) = Q(\\theta,\\theta^{(t)}) - \\mathbb{E}_{\\theta^{(t)}}[\\log p_{\\theta}(X|Y)|Y]\\,.\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\ell(Y;\\theta) - \\ell(Y;\\theta^{(t)}) = Q(\\theta,\\theta^{(t)})-Q(\\theta^{(t)},\\theta^{(t)}) + H(\\theta,\\theta^{(t)}) - H(\\theta^{(t)},\\theta^{(t)})\\,,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "H(\\theta,\\theta^{(t)}) = - \\mathbb{E}_{\\theta^{(t)}}[\\log p_{\\theta}(X|Y)|Y]\\,.\n",
    "$$\n",
    "\n",
    "The proof is completed by noting that\n",
    "\n",
    "$$\n",
    "H(\\theta,\\theta^{(t)}) - H(\\theta^{(t)},\\theta^{(t)})\\geqslant 0\\,,\n",
    "$$\n",
    "\n",
    "as this difference if a Kullback-Leibler divergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, $X = (X_1,\\ldots,X_n)$ and $Y = (Y_1,\\ldots,Y_n)$ where $\\{(X_i,Y_i)\\}_{1\\leqslant i\\leqslant n}$  are i.i.d. in $\\{-1,1\\} \\times \\mathbb{R}^d$. For $k\\in\\{-1,1\\}$, write $\\pi_k = \\mathbb{P}(X_1 = k)$. Assume that, conditionally on the event $\\{X_1 = k\\}$, $Y_1$ has a Gaussian distribution with mean $\\mu_k \\in\\mathbb{R}^d$ and covariance matrix $\\Sigma\\in \\mathbb{R}^{d\\times d}$. In this case, the parameter $\\theta=(\\pi_1, \\mu_1,\\mu_{-1}, \\Sigma)$ belongs to the set $\\Theta= [0,1] \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\mathbb{R}^{d \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Write the complete data loglikelihood </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete data loglikelihood  is given by\n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(X,Y\\right) = - \\frac{nd}{2} \\log(2\\pi)+\\sum_{i=1}^n\\sum_{k\\in\\{-1,1\\}}1_{X_i=k}\\left(\\log \\pi_{k} -\\frac{\\log \\det \\Sigma}{2} - \\frac{1}{2}\\left(Y_i - \\mu_{k}\\right)^T\\Sigma^{-1}\\left(Y_i - \\mu_{k}\\right)\\right)\\,,\n",
    "$$\n",
    "which yields\n",
    "$$\n",
    "\\log p_{\\theta}\\left(X,Y\\right) = - \\frac{nd}{2} \\log(2\\pi)-\\frac{n}2 \\log\\det\\Sigma + \\left(\\sum_{i=1}^n 1_{X_i=1}\\right)\\log \\pi_1 + \\left(\\sum_{i=1}^n 1_{X_i=-1}\\right)\\log (1-\\pi_{1}) -  \\frac{1}{2}\\sum_{i=1}^n 1_{X_i=1}\\left(Y_i - \\mu_{1}\\right)^T\\Sigma^{-1}\\left(Y_i - \\mu_{1}\\right) -  \\frac{1}{2}\\sum_{i=1}^n 1_{X_i=-1}\\left(Y_i - \\mu_{-1}\\right)^T\\Sigma^{-1}\\left(Y_i - \\mu_{-1}\\right)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Let $\\theta^{(t)}$ be the current parameter estimate. Compute $\\theta\\mapsto Q(\\theta,\\theta^{(t)})$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write $\\omega_t^i = \\mathbb{P}_{\\theta^{(t)}}(X_i=1|Y_i)$. The intermediate quantity of the EM algorithm is given by\n",
    "\n",
    "$$\n",
    "Q(\\theta,\\theta^{(t)}) = - \\frac{nd}{2} \\log(2\\pi)-\\frac{n}2 \\log\\det\\Sigma + \\left(\\sum_{i=1}^n\\omega_t^i \\right)\\log \\pi_1 + \\sum_{i=1}^n\\left(1 - \\omega_t^i \\right)\\log (1-\\pi_{1}) -  \\frac{1}{2}\\sum_{i=1}^n\\omega_t^i \\left(Y_i - \\mu_{1}\\right)^T\\Sigma^{-1}\\left(Y_i - \\mu_{1}\\right) -  \\frac{1}{2}\\sum_{i=1}^n(1-\\omega_t^i )\\left(Y_i - \\mu_{-1}\\right)^T\\Sigma^{-1}\\left(Y_i - \\mu_{-1}\\right)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Compute $\\theta^{(t+1)}$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $Q(\\theta,\\theta^{(t)})$ with respect to $\\theta$ is therefore given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q(\\theta,\\theta^{(t)})}{\\partial \\pi_1} = \\frac{\\sum_{i=1}^n\\omega_t^i}{\\pi_1} - \\frac{n-\\sum_{i=1}^n\\omega_t^i}{1-\\pi_{1}}\\,,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q(\\theta,\\theta^{(t)})}{\\partial \\mu_1} = \\sum_{i=1}^n\\omega_t^i\\left(2\\Sigma^{-1}Y_i - 2\\Sigma^{-1}\\mu_{1}\\right)\\,,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q(\\theta,\\theta^{(t)})}{\\partial \\mu_{-1}} = \\sum_{i=1}^n(1-\\omega_t^i)\\left(2\\Sigma^{-1}Y_i - 2\\Sigma^{-1}\\mu_{-1}\\right)\\,,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q(\\theta,\\theta^{(t)})}{\\partial \\Sigma^{-1}} = \\frac{n}{2}\\Sigma -  \\frac{1}{2}\\sum_{i=1}^n\\omega_t^i\\left(Y_i - \\mu_{1}\\right)\\left(Y_i - \\mu_{1}\\right)^T -  \\frac{1}{2}\\sum_{i=1}^n(1-\\omega_t^i)\\left(Y_i - \\mu_{-1}\\right)\\left(Y_i - \\mu_{-1}\\right)^T\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "Then, $\\theta^{(t+1)}$ is defined as the only parameter such that all these equations are set to 0. It is given by\n",
    "\n",
    "$$\n",
    "\\widehat \\pi_1^{(t+1)} = \\frac{1}{n}\\sum_{i=1}^n\\omega_t^i\\,,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat \\mu_1^{(t+1)} = \\frac{1}{\\sum_{i=1}^n\\omega_t^i}\\sum_{i=1}^n\\omega_t^i\\,Y_i\\,,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat\\Sigma^{(t+1)} = \\frac{1}{n}\\sum_{i=1}^n\\omega_t^i\\left(Y_i - \\mu_{1}\\right)\\left(Y_i - \\mu_{1}\\right)^T +  \\frac{1}{n}\\sum_{i=1}^n(1-\\omega_t^i)\\left(Y_i - \\mu_{-1}\\right)\\left(Y_i - \\mu_{-1}\\right)^T\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Extend these computations when the variance is not assumed to be the same in each group (homework).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange>  1. Simulated data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations\n",
    "n_samples = 100\n",
    "\n",
    "# means and variance to be estimated\n",
    "mu1, sigma1 = -2, 1.5\n",
    "mu2, sigma2 = 3, 1\n",
    "\n",
    "# prior probability of the first cluster/goup \n",
    "pi1 = 0.3\n",
    "\n",
    "x1 = np.random.normal(mu1, np.sqrt(sigma1), n_samples)\n",
    "x2 = np.random.normal(mu2, np.sqrt(sigma2), n_samples)\n",
    "\n",
    "X = np.zeros(n_samples)\n",
    "U = np.random.uniform(0,1,n_samples)\n",
    "for idx in range(n_samples):\n",
    "    if U[idx]<pi1:\n",
    "        X[idx] = x1[idx]\n",
    "    else:\n",
    "        X[idx] = x2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mean, variance):\n",
    "  z = np.exp(-(x - mean)**2/(2*variance))/np.sqrt(2*np.pi*variance)\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the training data\n",
    "bins = np.linspace(np.min(X),np.max(X),100)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.scatter(X, np.zeros(len(X)), color='orange', s=30, marker='o', label=\"Training data\")\n",
    "\n",
    "plt.plot(bins, Gaussian_pdf(bins, mu1, sigma1), linestyle = 'dashed', alpha = 0.5, color='red', label=\"True 1st density\")\n",
    "plt.plot(bins, Gaussian_pdf(bins, mu2, sigma2), linestyle = 'dashed', alpha = 0.5, color='k', label=\"True 2nd density\")\n",
    "plt.plot(bins, pi1*Gaussian_pdf(bins, mu1, sigma1) + (1-pi1)*Gaussian_pdf(bins, mu2, sigma2), alpha = 0.5, color='b', label=\"True mixture density\")\n",
    "\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange>  2. EM algorithm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot estimated density\n",
    "def plot_pdf(X, pi1, mu1, mu2, sigma1, sigma2, w, means, variances, it):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"probability density\")\n",
    "    plt.title(\"Iteration {}\".format(it))\n",
    "\n",
    "    plt.plot(bins, Gaussian_pdf(bins, mu1, sigma1), alpha = 0.8, color='red', label=\"True 1st density\")\n",
    "    plt.plot(bins, Gaussian_pdf(bins, mu2, sigma2), alpha = 0.8, color='k', label=\"True 2nd density\")\n",
    "\n",
    "    plt.plot(bins, Gaussian_pdf(bins, means[0], variances[0]), linestyle = 'dashed', alpha = 0.5, color='red', label=\"Estimated 1st density\")\n",
    "    plt.plot(bins, Gaussian_pdf(bins, means[1], variances[1]), linestyle = 'dashed', alpha = 0.5, color='k', label=\"Estimated 2nd density\")\n",
    "    \n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.grid('True')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"probability density\")\n",
    "    plt.title(\"Iteration {}\".format(it))\n",
    "    \n",
    "    plt.plot(bins, pi1*Gaussian_pdf(bins, mu1, sigma1) + (1-pi1)*Gaussian_pdf(bins, mu2, sigma2), alpha = 0.5, color='b', label=\"True mixture density\")\n",
    "    plt.plot(bins, w*Gaussian_pdf(bins, means[0], variances[0]) + (1-w)*Gaussian_pdf(bins, means[1], variances[1]), linestyle = 'dashed', alpha = 0.5, color='b', label=\"Estimated mixture density\")\n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.grid('True')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> In order to compute the E-step of the algorithm, it remains to compute $\\omega_t^i = \\mathbb{P}_{\\theta^{(t)}}(X_i=1|Y_i)$. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write, for all $1\\leqslant i\\leqslant n$,\n",
    "\n",
    "$$\n",
    "\\omega_t^i = \\mathbb{P}_{\\theta^{(t)}}(X_i=1|Y_i) = \\frac{\\hat\\pi^{(t)}_1 g^{(t)}_1(Y_i)}{\\hat\\pi^{(t)}_1 g^{(t)}_1(Y_i) + (1-\\hat\\pi^{(t)}_1) g^{(t)}_{-1}(Y_i)}\\,,\n",
    "$$\n",
    "\n",
    "where, for $j\\in\\{-1,1\\}$, $g^{(t)}_j$ is the likelihood (density) of the observation $Y$ given the event $\\{X=j\\}$ when the model is parameterized by $\\theta^{(t)}$ (Gaussian distribution with mean $\\mu_{j}^{(t)}$ and variance $\\Sigma$):\n",
    "\n",
    "$$\n",
    "g^{(t)}_j: y \\mapsto  \\mathrm{det}(2\\pi\\Sigma^{(t)})^{-1/2}\\times\\mathrm{exp}\\left\\{-(y-\\mu_j^{(t)})^T(\\Sigma^{(t)})^{-1}(y-\\mu_j^{(t)})/2\\right\\}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Write a loop which performs the EM algorithm for the Gaussian mixture model. </font>\n",
    "\n",
    "<font color=darkred>Display the value of the log-likelihood along the iterations. </font>\n",
    "    \n",
    "<font color=darkred> Display the value of the estimated parameters along the iterations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clust = 2\n",
    "weights = np.ones((n_clust)) / n_clust\n",
    "means = [0, 1]\n",
    "variances = [0.5, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations of the EM algorithm\n",
    "n_it = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi1_est = np.zeros(n_it+1)\n",
    "mu1_est = np.zeros(n_it+1)\n",
    "mu2_est = np.zeros(n_it+1)\n",
    "sigma1_est = np.zeros(n_it+1)\n",
    "sigma2_est = np.zeros(n_it+1)\n",
    "\n",
    "pi1_est[0] = weights[0]\n",
    "mu1_est[0] = means[0]\n",
    "mu2_est[0] = means[1]\n",
    "sigma1_est[0] = variances[0]\n",
    "sigma2_est[0] = variances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood = np.zeros(n_it)\n",
    "for it in range(n_it):\n",
    "    \n",
    "    if it%10 == 0:\n",
    "        plot_pdf(X, pi1, mu1, mu2, sigma1, sigma2, weights[0] ,means, variances, it)\n",
    "    \n",
    "    # Likelihood of all data for each cluster\n",
    "    likelihood = []\n",
    "    weightedlik = []\n",
    "    for j in range(n_clust):\n",
    "        likelihood.append(Gaussian_pdf(X, means[j], np.sqrt(variances[j])))\n",
    "        weightedlik.append(weights[j]*likelihood[j])\n",
    "    likelihood = np.array(likelihood)\n",
    "    weightedlik = np.array(weightedlik)\n",
    "    \n",
    "    \n",
    "    \n",
    "    w = [] \n",
    "    # E & M  steps \n",
    "    for j in range(n_clust):  \n",
    "        # copute posterior probabilities (w_{it})\n",
    "        w.append((weightedlik[j]) / (np.sum([weightedlik[i] for i in range(n_clust)], axis=0)))\n",
    "        # updage mean and variance\n",
    "        means[j] = np.sum(w[j] * X) / (np.sum(w[j]))\n",
    "        variances[j] = np.sum(w[j] * np.square(X - means[j])) / (np.sum(w[j]))\n",
    "    \n",
    "        # update the weights\n",
    "        weights[j] = np.mean(w[j])\n",
    "        \n",
    "    weightedlik = np.sum(weightedlik,1)\n",
    "    loglikelihood[it] = np.sum(np.log(weightedlik))/n_samples\n",
    "    \n",
    "    pi1_est[it+1] = weights[0]\n",
    "    mu1_est[it+1] = means[0]\n",
    "    mu2_est[it+1] = means[1]\n",
    "    sigma1_est[it+1] = variances[0]\n",
    "    sigma2_est[it+1] = variances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loglikelihood along iterations\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loglikelihood\")\n",
    "   \n",
    "plt.plot(range(np.size(loglikelihood)), loglikelihood, alpha = 0.8, color='red')\n",
    "   \n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot parameters along iterations\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Parameters values\")\n",
    "   \n",
    "plt.plot(range(n_it+1), pi1_est, linestyle = 'dashed', alpha = 0.8, color='red', label = '$\\pi_1$')\n",
    "plt.plot(range(n_it+1), mu1_est, linestyle = 'dashed', alpha = 0.8, color='k', label = '$\\mu_1$')\n",
    "plt.plot(range(n_it+1), mu2_est, linestyle = 'dashed', alpha = 0.8, color='k', label = '$\\mu_{-1}$')\n",
    "plt.plot(range(n_it+1), sigma1_est, linestyle = 'dashed', alpha = 0.8, color='orange', label = '$\\sigma_1$')\n",
    "plt.plot(range(n_it+1), sigma2_est, linestyle = 'dashed', alpha = 0.8, color='orange', label = '$\\sigma_{-1}$')\n",
    "\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Explore the sensitivity of the algorithm with respect to i) the initial estimates, ii) the number of iterations, iii) the number of observations and iv) the number of clusters. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
